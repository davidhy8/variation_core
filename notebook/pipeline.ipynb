{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "608ed739-c748-4282-a2df-7a8af47818bb",
   "metadata": {},
   "source": [
    "# Paper flagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe099315-a725-415e-b9fd-0ad9e324e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/david.yang1/.cache/huggingface/'\n",
    "os.environ['HF_HOME'] = '/home/david.yang1/.cache/huggingface/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2daaaf9-b7aa-4b88-8731-117600c28586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david.yang1/.local/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-07-25 09:20:11.870506: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-25 09:20:13.806650: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-25 09:20:13.806686: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-25 09:20:13.815510: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-25 09:20:14.834677: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-25 09:20:19.073862: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, pipeline, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "# from huggingface_hub import login\n",
    "import evaluate\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from torch.nn import functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4340054-f128-4268-ac4a-1c06b8ca056f",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2438d10-ed93-4005-8110-84f9a48c39fb",
   "metadata": {},
   "source": [
    "### Load training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "936dd18c-ad8f-4fa3-8971-cf2ec58a8f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david.yang1/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load BioBERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NeuML/pubmedbert-base-embeddings\")\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(df):\n",
    "    return tokenizer(\n",
    "        df['text'],\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        max_length = 512\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6df581e6-362f-4698-8022-ce12b4e66de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_preparation(df, val_count=0):\n",
    "    # Balance classes if needed\n",
    "    df = df.groupby('label').sample(n=min(df['label'].value_counts()), random_state=42)\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    df = df.sample(frac=1, random_state=42)\n",
    "    df = df[[\"text\", \"label\"]]\n",
    "    \n",
    "    # Split dataset into test & train\n",
    "    df_train = df[val_count:]\n",
    "    df_val = df[:val_count]\n",
    "    \n",
    "    tds = Dataset.from_pandas(df_train)\n",
    "    vds = Dataset.from_pandas(df_val)\n",
    "\n",
    "    # Apply the tokenizer to the datasets\n",
    "    tds = tds.map(tokenize_function, batched=True)\n",
    "    vds = vds.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Set the format of the datasets to include only the required columns\n",
    "    tds = tds.rename_column('__index_level_0__', 'index').remove_columns(['text', 'index'])\n",
    "    vds = vds.rename_column('__index_level_0__', 'index').remove_columns(['text', 'index'])\n",
    "    \n",
    "    # Define DatasetDict\n",
    "    ds = DatasetDict({\n",
    "        \"train\": tds,\n",
    "        \"validation\": vds\n",
    "    })\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd07a8d-89f1-4c65-96ca-f649ecb71544",
   "metadata": {},
   "source": [
    "### Model fine tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4991a887-017e-42c9-9348-00fb299443f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc5b9950-b36c-47f0-bf4e-74cfaf319ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune model\n",
    "def fine_tune_model(ds, model_init, train=False):\n",
    "    # Define the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        evaluation_strategy = \"steps\",\n",
    "        eval_steps=500,\n",
    "        num_train_epochs=3,    # number of training epochs\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_ratio=0.01,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "    )\n",
    "\n",
    "    # Create the Trainer and start training\n",
    "    trainer = Trainer(\n",
    "        args=training_args,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds[\"validation\"],\n",
    "        model_init=model_init,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    if train:\n",
    "        trainer.train()\n",
    "\n",
    "    if ds[\"validation\"]:\n",
    "        trainer.evaluate()\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86482e9f-cee7-421f-b90b-e32dc0be10fb",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19e53981-bcdc-4cec-bbf9-90fac21d6aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into <512 token chunks\n",
    "def split_text_into_chunks(text, tokenizer, max_tokens=512, overlap_sentences=2):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Initialize variables\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_len = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Tokenize the sentence using BERT Tokenizer\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        token_count = len(tokens)\n",
    "\n",
    "        # Finalize the current chunk if adding this sentence exceed token limit\n",
    "        if current_chunk_len + token_count > max_tokens:\n",
    "            text_chunk = \" \".join(current_chunk)\n",
    "            chunks.append(text_chunk)\n",
    "\n",
    "            # Create the next chunk with overlap\n",
    "            overlap_start = max(0, i-overlap_sentences)\n",
    "            current_chunk = []\n",
    "            for j in range(overlap_start, i):\n",
    "                current_chunk.append(sentences[j])\n",
    "            current_chunk_len = len(current_chunk)\n",
    "\n",
    "        # Add the current sentence tokens to the chunk\n",
    "        current_chunk.append(sentence)\n",
    "        current_chunk_len += token_count\n",
    "\n",
    "    # Add the last chunk if it has content\n",
    "    if current_chunk:\n",
    "        text_chunk = \" \".join(current_chunk)\n",
    "        chunks.append(text_chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d9793bb-d2bc-47b7-8a8c-2a5f28a38ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict label of dataframe\n",
    "def prediction_chunks(df, tokenizer, trainer):\n",
    "    output = pd.DataFrame()\n",
    "    for i, text in enumerate(df[\"text\"]):\n",
    "        chunks = split_text_into_chunks(text, tokenizer)\n",
    "        \n",
    "        chunks_df = pd.DataFrame(chunks, columns=[\"text\"])\n",
    "        # chunks_df[\"label\"] = df[\"label\"][i]\n",
    "        chunks_df[\"position\"] = chunks_df.index\n",
    "        chunks_df[\"pmid\"] = df.loc[i, \"pmid\"]\n",
    "        \n",
    "        t = Dataset.from_pandas(chunks_df)\n",
    "        t = t.map(tokenize_function, batched=True)\n",
    "        ds_t = DatasetDict({\n",
    "            \"test\": t\n",
    "        })\n",
    "\n",
    "        pred = trainer.predict(ds_t[\"test\"])\n",
    "        chunks_df[\"prediction\"] = pred.predictions.argmax(-1)\n",
    "\n",
    "        # convert logit score to torch array\n",
    "        torch_logits = torch.from_numpy(pred.predictions)\n",
    "\n",
    "        # get probabilities using softmax from logit score and convert it to numpy array\n",
    "        probabilities_scores = F.softmax(torch_logits, dim = -1).numpy()\n",
    "\n",
    "        chunks_df[\"probability\"] = probabilities_scores.max(-1)\n",
    "\n",
    "        # save into output\n",
    "        output = pd.concat([output, chunks_df], ignore_index=True)\n",
    "        \n",
    "    return output, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac674e-0e4b-4b6f-8f51-3112c3a835de",
   "metadata": {},
   "source": [
    "### Dataset to validate chunk prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22be07ee-b6e7-4ddf-b3f4-0025c5b3c55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load prediction chunks\n",
    "# pred_chunks_0 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/0_chunks_labelled.csv\")\n",
    "# pred_chunks_1 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/1_chunks_labelled.csv\")\n",
    "# pred_chunks_2 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/2_chunks_labelled.csv\")\n",
    "# pred_chunks_3 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/3_chunks_labelled.csv\")\n",
    "# pred_chunks_4 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/4_chunks_labelled.csv\")\n",
    "# pred_chunks_5 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/5_chunks_labelled.csv\")\n",
    "# pred_chunks_6 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/6_chunks_labelled.csv\")\n",
    "# pred_chunks_7 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/7_chunks_labelled.csv\")\n",
    "# pred_chunks_8 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/8_chunks_labelled.csv\")\n",
    "# pred_chunks_9 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/9_chunks_labelled.csv\")\n",
    "# pred_chunks_10 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/10_chunks_labelled.csv\")\n",
    "# pred_chunks_11 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/11_chunks_labelled.csv\")\n",
    "\n",
    "\n",
    "# # Concatenate data\n",
    "# df_test = pd.concat([pred_chunks_0, pred_chunks_1, pred_chunks_2, pred_chunks_3, pred_chunks_4, pred_chunks_5, pred_chunks_6, pred_chunks_7, \n",
    "#                      pred_chunks_8, pred_chunks_9, pred_chunks_10, pred_chunks_11])\n",
    "\n",
    "# # Load dataframe as dataset\n",
    "# test = Dataset.from_pandas(df_test)\n",
    "\n",
    "# # Tokenize test dataset\n",
    "# test = test.map(tokenize_function, batched=True)\n",
    "\n",
    "# # Set the format of the datasets to include only the required columns\n",
    "# test = test.rename_column('__index_level_0__', 'index').remove_columns(['text', 'index'])\n",
    "\n",
    "# # Define DatasetDict\n",
    "# ds_test = DatasetDict({\n",
    "#     \"test\": test\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "572b20e2-b847-46fc-a6a5-1e2f2035bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(trainer):\n",
    "    # Load prediction chunks\n",
    "    pred_chunks_0 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/0_chunks_labelled.csv\")\n",
    "    pred_chunks_1 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/1_chunks_labelled.csv\")\n",
    "    pred_chunks_2 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/2_chunks_labelled.csv\")\n",
    "    pred_chunks_3 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/3_chunks_labelled.csv\")\n",
    "    pred_chunks_4 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/4_chunks_labelled.csv\")\n",
    "    pred_chunks_5 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/5_chunks_labelled.csv\")\n",
    "    pred_chunks_6 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/6_chunks_labelled.csv\")\n",
    "    pred_chunks_7 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/7_chunks_labelled.csv\")\n",
    "    pred_chunks_8 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/8_chunks_labelled.csv\")\n",
    "    pred_chunks_9 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/9_chunks_labelled.csv\")\n",
    "    pred_chunks_10 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/10_chunks_labelled.csv\")\n",
    "    pred_chunks_11 = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/11_chunks_labelled.csv\")\n",
    "    \n",
    "    \n",
    "    # Concatenate data\n",
    "    df_test = pd.concat([pred_chunks_0, pred_chunks_1, pred_chunks_2, pred_chunks_3, pred_chunks_4, pred_chunks_5, pred_chunks_6, pred_chunks_7, \n",
    "                         pred_chunks_8, pred_chunks_9, pred_chunks_10, pred_chunks_11])\n",
    "    \n",
    "    # Load dataframe as dataset\n",
    "    test = Dataset.from_pandas(df_test)\n",
    "    \n",
    "    # Tokenize test dataset\n",
    "    test = test.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Set the format of the datasets to include only the required columns\n",
    "    test = test.rename_column('__index_level_0__', 'index').remove_columns(['text', 'index'])\n",
    "    \n",
    "    # Define DatasetDict\n",
    "    ds_test = DatasetDict({\n",
    "        \"test\": test\n",
    "    })\n",
    "    \n",
    "    # Test performance of the model on labeled chunks\n",
    "    pred = trainer.predict(ds_test[\"test\"])\n",
    "    \n",
    "    df_test[\"prediction\"] = pred.predictions.argmax(-1)\n",
    "    \n",
    "    metrics = compute_metrics(pred)\n",
    "\n",
    "    return metrics    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bed027-9924-4660-8ad7-c3fa05bc1085",
   "metadata": {},
   "source": [
    "## Chunk labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3220db65-f793-4cbc-899d-22d20f10d4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9a452b53894bc9a929c63573d4d406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9022f30f8dd5430da8253dd919259b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david.yang1/.local/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f0dd46190b4b968f629ae7780779ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.68, 'f1': 0.6875000000000001, 'precision': 0.5301204819277109, 'recall': 0.9777777777777777}\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../data/pipeline_data/paper_flagging_data/bert_dataset.csv')\n",
    "ds = ds_preparation(df, val_count=128)\n",
    "\n",
    "# Load model\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"../checkpoints/chunks-pubmed-bert-v2\", num_labels=2)\n",
    "\n",
    "trainer = fine_tune_model(ds, model_init, train=False)\n",
    "\n",
    "# Check performance\n",
    "metrics = validate_model(trainer)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82585ea9-795d-4edb-9bfb-d3c72298def9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6dbfbd6a5ec488e9c4e23f9cd7b830f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3158b74fca4251a6b8c22f71662253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4b6a01a2274a9fa1b22c3df35076ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3eff25ec2246f096c28b24478ec89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27422698a82a4906875cce22a7b71b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a897de1559d1495e9277897628661969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75083e7abf749f1a613a8047d339027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d644b0d1e04edeabcc653192e41d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fc6e159a8f48488a67f87973efb1a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49afc24f88724947bbc02b6ea8b42b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load new papers\n",
    "# data = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/new_papers_dataset.csv\")\n",
    "data = pd.read_csv(\"../data/pipeline_data/test.csv\")\n",
    "chunked_data_df, chunked_data_pred = prediction_chunks(data[:10], tokenizer, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa45885c-9f60-46ea-b558-48979d8d36f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>position</th>\n",
       "      <th>pmid</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rapid detection and tracking of Omicron varian...</td>\n",
       "      <td>0</td>\n",
       "      <td>35189535</td>\n",
       "      <td>1</td>\n",
       "      <td>0.959968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The new features of Omicron manifested the imp...</td>\n",
       "      <td>1</td>\n",
       "      <td>35189535</td>\n",
       "      <td>1</td>\n",
       "      <td>0.944278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Plasmid construction and virus RNA of SARS-CoV...</td>\n",
       "      <td>2</td>\n",
       "      <td>35189535</td>\n",
       "      <td>1</td>\n",
       "      <td>0.993587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Furthermore, five oropharyngeal swab specimens...</td>\n",
       "      <td>3</td>\n",
       "      <td>35189535</td>\n",
       "      <td>1</td>\n",
       "      <td>0.938198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Workflow of RT-PCR/CRISPR-Cas12a-mediated assa...</td>\n",
       "      <td>4</td>\n",
       "      <td>35189535</td>\n",
       "      <td>1</td>\n",
       "      <td>0.976641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Immune dysregulation and autoreactivity correl...</td>\n",
       "      <td>40</td>\n",
       "      <td>35214706</td>\n",
       "      <td>0</td>\n",
       "      <td>0.994895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Optimizing immunization strategies for the ind...</td>\n",
       "      <td>41</td>\n",
       "      <td>35214706</td>\n",
       "      <td>0</td>\n",
       "      <td>0.877917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Vaccine Breakthrough Infections with SARS-CoV-...</td>\n",
       "      <td>42</td>\n",
       "      <td>35214706</td>\n",
       "      <td>1</td>\n",
       "      <td>0.992595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Photodynamic Vaccination of BALB/c Mice for Pr...</td>\n",
       "      <td>43</td>\n",
       "      <td>35214706</td>\n",
       "      <td>0</td>\n",
       "      <td>0.950951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Schematic depiction of transgenic production o...</td>\n",
       "      <td>44</td>\n",
       "      <td>35214706</td>\n",
       "      <td>0</td>\n",
       "      <td>0.973186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  position      pmid  \\\n",
       "0    Rapid detection and tracking of Omicron varian...         0  35189535   \n",
       "1    The new features of Omicron manifested the imp...         1  35189535   \n",
       "2    Plasmid construction and virus RNA of SARS-CoV...         2  35189535   \n",
       "3    Furthermore, five oropharyngeal swab specimens...         3  35189535   \n",
       "4    Workflow of RT-PCR/CRISPR-Cas12a-mediated assa...         4  35189535   \n",
       "..                                                 ...       ...       ...   \n",
       "195  Immune dysregulation and autoreactivity correl...        40  35214706   \n",
       "196  Optimizing immunization strategies for the ind...        41  35214706   \n",
       "197  Vaccine Breakthrough Infections with SARS-CoV-...        42  35214706   \n",
       "198  Photodynamic Vaccination of BALB/c Mice for Pr...        43  35214706   \n",
       "199  Schematic depiction of transgenic production o...        44  35214706   \n",
       "\n",
       "     prediction  probability  \n",
       "0             1     0.959968  \n",
       "1             1     0.944278  \n",
       "2             1     0.993587  \n",
       "3             1     0.938198  \n",
       "4             1     0.976641  \n",
       "..          ...          ...  \n",
       "195           0     0.994895  \n",
       "196           0     0.877917  \n",
       "197           1     0.992595  \n",
       "198           0     0.950951  \n",
       "199           0     0.973186  \n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed2276a-109c-44b1-af3f-ec5354bb1262",
   "metadata": {},
   "source": [
    "## Paper classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "810b9d18-9cf7-42b8-b8b5-478d4f08960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f23b54a3-37b9-4374-b009-d49d032fd79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paper_prediction(data, bst, tokenizer, trainer):\n",
    "    chunked_data_df, chunked_data_pred = prediction_chunks(data, tokenizer, trainer)\n",
    "    \n",
    "    # Load lightbgm model\n",
    "    # bst = lgb.Booster(model_file)\n",
    "\n",
    "    # Format lightGBM data \n",
    "    data = chunked_data_df\n",
    "    \n",
    "    # Format data for lightGBM\n",
    "    grouped = data.groupby('pmid')\n",
    "    \n",
    "    # Maximum number of data points in any group\n",
    "    max_len = 133\n",
    "\n",
    "    # Create DataFrame with appropriate number of columns\n",
    "    columns = [f'prediction_{i}' for i in range(max_len)]\n",
    "    columns.append(\"pmid\")\n",
    "    \n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for name, group in grouped:\n",
    "        predictions = group[\"prediction\"].values.astype(float)\n",
    "        entry = np.pad(predictions, (0, max_len - len(predictions)), constant_values=np.nan)\n",
    "        entry = np.append(entry, name)\n",
    "        df.loc[name] = entry\n",
    "\n",
    "    predictions = bst.predict(df.drop(columns=\"pmid\"), num_iteration=bst.best_iteration)\n",
    "    pred = np.where(predictions < 0.5, 0, 1)\n",
    "    df[\"prediction\"] = pred.T\n",
    "\n",
    "    df['pmid'] = df['pmid'].astype(int)\n",
    "    flagged_papers = df[df['prediction'] == 1]['pmid']\n",
    "    \n",
    "    relevant_papers = flagged_papers.tolist()\n",
    "    flagged = chunked_data_df[chunked_data_df[\"pmid\"].isin(relevant_papers)]\n",
    "\n",
    "    return flagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f83c737c-7c69-4a8e-a45d-9924a41cee9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53aad0f041b443290b17b5e8985a150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d98bb7232a64de1905d3ec88b013962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david.yang1/.local/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb7b4b19fee48c68cdd121d4225e115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e725b55cffcc40e79ef6a285c713f3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9dd09bb99794719b4c4a9ab232f559a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5044ab2781f243b0b950aa041042af02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e1db30006f4790be6308d6ab77f724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91c688bdebe4cb6849ebac6b3a386bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e73de99119c4e28975d82d8ff3dd4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66a21ca5f844c498556e8269155549f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af8fba544064f10ba59c799b7ea76cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a5763402ea46ab9f624ced86a207fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data = pd.read_csv(\"../data/pipeline_data/paper_flagging_data/new_papers_dataset.csv\")\n",
    "data = pd.read_csv(\"../data/pipeline_data/test.csv\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('../data/pipeline_data/paper_flagging_data/bert_dataset.csv')\n",
    "ds = ds_preparation(df, val_count=128)\n",
    "\n",
    "# Load model\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"../checkpoints/chunks-pubmed-bert-v2\", num_labels=2)\n",
    "\n",
    "trainer = fine_tune_model(ds, model_init, train=False)\n",
    "\n",
    "results = paper_prediction(data[:10], bst, tokenizer, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0040e94-7bf0-4db5-b3fb-adcc73edb661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>position</th>\n",
       "      <th>pmid</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rapid detection and tracking of Omicron varian...</td>\n",
       "      <td>0</td>\n",
       "      <td>35189535</td>\n",
       "      <td>1</td>\n",
       "      <td>0.959968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The new features of Omicron manifested the imp...</td>\n",
       "      <td>1</td>\n",
       "      <td>35189535</td>\n",
       "      <td>1</td>\n",
       "      <td>0.944278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Plasmid construction and virus RNA of SARS-CoV...</td>\n",
       "      <td>2</td>\n",
       "      <td>35189535</td>\n",
       "      <td>1</td>\n",
       "      <td>0.993587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Furthermore, five oropharyngeal swab specimens...</td>\n",
       "      <td>3</td>\n",
       "      <td>35189535</td>\n",
       "      <td>1</td>\n",
       "      <td>0.938198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Workflow of RT-PCR/CRISPR-Cas12a-mediated assa...</td>\n",
       "      <td>4</td>\n",
       "      <td>35189535</td>\n",
       "      <td>1</td>\n",
       "      <td>0.976641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Finally, the fluorescence signal was measured ...</td>\n",
       "      <td>5</td>\n",
       "      <td>35189535</td>\n",
       "      <td>1</td>\n",
       "      <td>0.986942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>In all panels, error bars represent the mean +...</td>\n",
       "      <td>6</td>\n",
       "      <td>35189535</td>\n",
       "      <td>1</td>\n",
       "      <td>0.987321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2B-E). To estimate the detection limit of our ...</td>\n",
       "      <td>7</td>\n",
       "      <td>35189535</td>\n",
       "      <td>1</td>\n",
       "      <td>0.985151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Furthermore, a relatively lower signal was obs...</td>\n",
       "      <td>8</td>\n",
       "      <td>35189535</td>\n",
       "      <td>1</td>\n",
       "      <td>0.985241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>In our study, Omicron and wild-type targets we...</td>\n",
       "      <td>9</td>\n",
       "      <td>35189535</td>\n",
       "      <td>1</td>\n",
       "      <td>0.891522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Two-step strategy for the identification of SA...</td>\n",
       "      <td>10</td>\n",
       "      <td>35189535</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>S2. The testing results of SARS-CoV-2 positive...</td>\n",
       "      <td>11</td>\n",
       "      <td>35189535</td>\n",
       "      <td>1</td>\n",
       "      <td>0.981975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Comparison of SARS-CoV-2 Evolution in Paediatr...</td>\n",
       "      <td>0</td>\n",
       "      <td>35215919</td>\n",
       "      <td>1</td>\n",
       "      <td>0.517193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Introduction. Severe acute respiratory syndrom...</td>\n",
       "      <td>1</td>\n",
       "      <td>35215919</td>\n",
       "      <td>0</td>\n",
       "      <td>0.992498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>A seemingly unique feature of SARS-CoV-2 is th...</td>\n",
       "      <td>2</td>\n",
       "      <td>35215919</td>\n",
       "      <td>1</td>\n",
       "      <td>0.974569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2.2. WD-PNECs. Nasal epithelial cells from pre...</td>\n",
       "      <td>3</td>\n",
       "      <td>35215919</td>\n",
       "      <td>0</td>\n",
       "      <td>0.976566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>On the day of titration, the growth media was ...</td>\n",
       "      <td>4</td>\n",
       "      <td>35215919</td>\n",
       "      <td>0</td>\n",
       "      <td>0.960336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Reactions were incubated at 42  C (50 min) and...</td>\n",
       "      <td>5</td>\n",
       "      <td>35215919</td>\n",
       "      <td>1</td>\n",
       "      <td>0.988121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>To begin to understand the evolution of SARS-C...</td>\n",
       "      <td>6</td>\n",
       "      <td>35215919</td>\n",
       "      <td>1</td>\n",
       "      <td>0.991886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Sequencing of SARS-CoV-2 Passage Series in Unm...</td>\n",
       "      <td>7</td>\n",
       "      <td>35215919</td>\n",
       "      <td>1</td>\n",
       "      <td>0.993632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>These changes mapped to several genes and prot...</td>\n",
       "      <td>8</td>\n",
       "      <td>35215919</td>\n",
       "      <td>1</td>\n",
       "      <td>0.993639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>For both PHE and BT20.1, robust infection and ...</td>\n",
       "      <td>9</td>\n",
       "      <td>35215919</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Phenotypic Differences between SARS-CoV-2 PHE ...</td>\n",
       "      <td>10</td>\n",
       "      <td>35215919</td>\n",
       "      <td>1</td>\n",
       "      <td>0.993681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Complementary to this, in vitro systems are an...</td>\n",
       "      <td>11</td>\n",
       "      <td>35215919</td>\n",
       "      <td>1</td>\n",
       "      <td>0.990983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>P812R is a non-conservative change and rapidly...</td>\n",
       "      <td>12</td>\n",
       "      <td>35215919</td>\n",
       "      <td>1</td>\n",
       "      <td>0.993917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>ORF7A truncations in SARS-CoV-2 isolates have ...</td>\n",
       "      <td>13</td>\n",
       "      <td>35215919</td>\n",
       "      <td>1</td>\n",
       "      <td>0.992569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>It is likely that the evolution of key mutatio...</td>\n",
       "      <td>14</td>\n",
       "      <td>35215919</td>\n",
       "      <td>1</td>\n",
       "      <td>0.993582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Reference is cited in the supplementary materi...</td>\n",
       "      <td>15</td>\n",
       "      <td>35215919</td>\n",
       "      <td>1</td>\n",
       "      <td>0.924114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>All authors have read and agreed to the publis...</td>\n",
       "      <td>16</td>\n",
       "      <td>35215919</td>\n",
       "      <td>0</td>\n",
       "      <td>0.987381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>A Multibasic Cleavage Site in the Spike Protei...</td>\n",
       "      <td>17</td>\n",
       "      <td>35215919</td>\n",
       "      <td>0</td>\n",
       "      <td>0.718202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>A Replication-Competent Vesicular Stomatitis V...</td>\n",
       "      <td>18</td>\n",
       "      <td>35215919</td>\n",
       "      <td>1</td>\n",
       "      <td>0.991801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Only sequences from P1-P4 (PHE) and P2-P4 (BT2...</td>\n",
       "      <td>19</td>\n",
       "      <td>35215919</td>\n",
       "      <td>1</td>\n",
       "      <td>0.993069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  position      pmid  \\\n",
       "0    Rapid detection and tracking of Omicron varian...         0  35189535   \n",
       "1    The new features of Omicron manifested the imp...         1  35189535   \n",
       "2    Plasmid construction and virus RNA of SARS-CoV...         2  35189535   \n",
       "3    Furthermore, five oropharyngeal swab specimens...         3  35189535   \n",
       "4    Workflow of RT-PCR/CRISPR-Cas12a-mediated assa...         4  35189535   \n",
       "5    Finally, the fluorescence signal was measured ...         5  35189535   \n",
       "6    In all panels, error bars represent the mean +...         6  35189535   \n",
       "7    2B-E). To estimate the detection limit of our ...         7  35189535   \n",
       "8    Furthermore, a relatively lower signal was obs...         8  35189535   \n",
       "9    In our study, Omicron and wild-type targets we...         9  35189535   \n",
       "10   Two-step strategy for the identification of SA...        10  35189535   \n",
       "11   S2. The testing results of SARS-CoV-2 positive...        11  35189535   \n",
       "135  Comparison of SARS-CoV-2 Evolution in Paediatr...         0  35215919   \n",
       "136  Introduction. Severe acute respiratory syndrom...         1  35215919   \n",
       "137  A seemingly unique feature of SARS-CoV-2 is th...         2  35215919   \n",
       "138  2.2. WD-PNECs. Nasal epithelial cells from pre...         3  35215919   \n",
       "139  On the day of titration, the growth media was ...         4  35215919   \n",
       "140  Reactions were incubated at 42  C (50 min) and...         5  35215919   \n",
       "141  To begin to understand the evolution of SARS-C...         6  35215919   \n",
       "142  Sequencing of SARS-CoV-2 Passage Series in Unm...         7  35215919   \n",
       "143  These changes mapped to several genes and prot...         8  35215919   \n",
       "144  For both PHE and BT20.1, robust infection and ...         9  35215919   \n",
       "145  Phenotypic Differences between SARS-CoV-2 PHE ...        10  35215919   \n",
       "146  Complementary to this, in vitro systems are an...        11  35215919   \n",
       "147  P812R is a non-conservative change and rapidly...        12  35215919   \n",
       "148  ORF7A truncations in SARS-CoV-2 isolates have ...        13  35215919   \n",
       "149  It is likely that the evolution of key mutatio...        14  35215919   \n",
       "150  Reference is cited in the supplementary materi...        15  35215919   \n",
       "151  All authors have read and agreed to the publis...        16  35215919   \n",
       "152  A Multibasic Cleavage Site in the Spike Protei...        17  35215919   \n",
       "153  A Replication-Competent Vesicular Stomatitis V...        18  35215919   \n",
       "154  Only sequences from P1-P4 (PHE) and P2-P4 (BT2...        19  35215919   \n",
       "\n",
       "     prediction  probability  \n",
       "0             1     0.959968  \n",
       "1             1     0.944278  \n",
       "2             1     0.993587  \n",
       "3             1     0.938198  \n",
       "4             1     0.976641  \n",
       "5             1     0.986942  \n",
       "6             1     0.987321  \n",
       "7             1     0.985151  \n",
       "8             1     0.985241  \n",
       "9             1     0.891522  \n",
       "10            1     0.994054  \n",
       "11            1     0.981975  \n",
       "135           1     0.517193  \n",
       "136           0     0.992498  \n",
       "137           1     0.974569  \n",
       "138           0     0.976566  \n",
       "139           0     0.960336  \n",
       "140           1     0.988121  \n",
       "141           1     0.991886  \n",
       "142           1     0.993632  \n",
       "143           1     0.993639  \n",
       "144           1     0.995845  \n",
       "145           1     0.993681  \n",
       "146           1     0.990983  \n",
       "147           1     0.993917  \n",
       "148           1     0.992569  \n",
       "149           1     0.993582  \n",
       "150           1     0.924114  \n",
       "151           0     0.987381  \n",
       "152           0     0.718202  \n",
       "153           1     0.991801  \n",
       "154           1     0.993069  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9eef3aa1-50ac-4e2f-af12-cbd792635a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lightbgm model\n",
    "bst = lgb.Booster(model_file='../checkpoints/lightgbm_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16c3b5a-0c20-4cb8-8b38-3d604c9a5c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format lightGBM data \n",
    "data = chunked_data_df\n",
    "\n",
    "# Format data for lightGBM\n",
    "grouped = data.groupby('paper')\n",
    "\n",
    "# Maximum number of data points in any group\n",
    "max_len = 133\n",
    "# max_len = max(grouped.size())\n",
    "# print(max_len)\n",
    "\n",
    "# Create DataFrame with appropriate number of columns\n",
    "columns = [f'prediction_{i}' for i in range(max_len)]\n",
    "columns.append(\"paper\")\n",
    "\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for name, group in grouped:\n",
    "    predictions = group[\"prediction\"].values.astype(float)\n",
    "    entry = np.pad(predictions, (0, max_len - len(predictions)), constant_values=np.nan)\n",
    "    # entry = np.pad(predictions, (0, 133 - len(predictions)), constant_values=np.nan)\n",
    "    entry = np.append(entry, name)\n",
    "    df.loc[name] = entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de0e9d7-a3a3-4a00-9867-4da224fd33be",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = bst.predict(df.drop(columns=\"paper\"), num_iteration=bst.best_iteration)\n",
    "pred = np.where(predictions < 0.5, 0, 1)\n",
    "df[\"prediction\"] = pred.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04401b9e-6e53-4280-9a42-e6f51c9fe567",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['paper'] = df['paper'].astype(int)\n",
    "flagged_papers = df[df['prediction'] == 1]['paper']\n",
    "\n",
    "relevant_papers = flagged_papers.tolist()\n",
    "flagged = chunked_data_df[chunked_data_df[\"paper\"].isin(relevant_papers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5137b2e0-db2f-4e72-8865-677d01437404",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flagged[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48393b1-555a-4e82-a00d-f9d81112f2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flagged[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2aaf18-50c2-443f-852c-e3dc4da56320",
   "metadata": {},
   "source": [
    "# NER with BERN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ea1965d-b4ca-453e-b518-b0b2575fc230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9027009-3f45-47d9-a492-e5b6cbf651d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "flagged = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "079dd373-b6cb-47a6-b06f-ab47e8e0b9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_plain(text, url=\"http://localhost:8888/plain\"):\n",
    "    return requests.post(url, json={'text': text}).json()\n",
    "\n",
    "# port = \"http://172.19.8.251:8888/plain\"\n",
    "port = \"http://172.19.4.131:8888/plain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "541acd59-63ce-4169-a1ef-283eeea26d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_papers = flagged.groupby('pmid')\n",
    "\n",
    "for name, group in grouped_papers:\n",
    "    NER_list = list()\n",
    "    for text in group[\"text\"]:\n",
    "        NER = query_plain(text, url = port)\n",
    "        NER_list.append(NER)\n",
    "    file_name = \"../data/pipeline_data/NER/\" + str(name) + \"_paper.pkl\"\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(NER_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e36a9-c8db-4322-a931-bcda88fee524",
   "metadata": {},
   "source": [
    "# Figure out what each mutation does\n",
    "https://www.reddit.com/r/MachineLearning/comments/o0kixr/improving_bart_text_summarization_by_providing/\n",
    "\n",
    "https://peterbloem.nl/blog/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9954705-8052-41e1-8ab4-350b56c572d4",
   "metadata": {},
   "source": [
    "# Description of mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8c55a46-8520-4419-aeaf-247edbfa0961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import spacy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e71d36e4-b0db-40b0-ba1f-b8a8b324e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f150f0b9-a402-40ea-b1c3-82849e86d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = Path(\"/home/david.yang1/autolit/viriation/data/pipeline_data/NER\").glob(\"*.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3acaa88-5efa-4de9-93fa-d62247c704c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mutations(path = \"/home/david.yang1/autolit/viriation/data/pipeline_data/NER\"):\n",
    "    files = Path(path).glob(\"*.pkl\")\n",
    "    for file in files:\n",
    "        # Initialize output dictionary\n",
    "        output = defaultdict(lambda:{\"doi\": [], \"text\": []})\n",
    "        \n",
    "        with open(file, 'rb') as f:\n",
    "            # Load NERs\n",
    "            ner = pickle.load(f) \n",
    "\n",
    "            for ner_chunk in ner:\n",
    "                text = ner_chunk['text']\n",
    "\n",
    "                # Process text chunks for increased readability\n",
    "                chunk = nlp(text)\n",
    "\n",
    "                # Split text into sentences\n",
    "                sentences = [sent.text for sent in chunk.sents]\n",
    "                annotations = ner_chunk['annotations']\n",
    "                \n",
    "                for annotation in annotations:\n",
    "                    if annotation['obj'] == 'mutation':\n",
    "                        mutation = annotation[\"mention\"]\n",
    "                        for count, sent in enumerate(sentences):\n",
    "                            if mutation in sent:\n",
    "                                context = []\n",
    "\n",
    "                                # Save sentence before and after mutation\n",
    "                                if count != 0:\n",
    "                                    context.append(sentences[count-1])\n",
    "    \n",
    "                                context.append(sent)\n",
    "\n",
    "                                if count != (len(sentences)-1):\n",
    "                                    context.append(sentences[count+1])\n",
    "    \n",
    "                                context = \" \".join(context)\n",
    "    \n",
    "                                output[mutation][\"text\"].append(context) \n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d81f5c9-418c-4a67-bcd7-b26f5675035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = load_mutations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e44e02e8-abfb-4a41-a45d-5eb135786ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The newly emerged fifth variant of concern (VOC) Omicron was firstly reported in South Africa on November 24, 2021 and has been detected in many countries. Omicron variant contains more than 32 amino acid mutations in the spike protein, including multiple vital amino acid mutations (K417N, T478K, E484A, N501Y, and D614G) that have been already detected in other VOCs of SARS-CoV-2 and proved to be associated with enhanced transmissibility, virulence, and greater resistance to the immune protection induced by COVID-19 vaccines. The new features of Omicron manifested the importance of tracking its spread.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"K417N\"][\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3f93ac-8a7e-4fe4-ae5c-3adce448b540",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
