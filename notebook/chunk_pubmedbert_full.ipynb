{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe099315-a725-415e-b9fd-0ad9e324e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/david.yang1/.cache/huggingface/'\n",
    "os.environ['HF_HOME'] = '/home/david.yang1/.cache/huggingface/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2daaaf9-b7aa-4b88-8731-117600c28586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david.yang1/.local/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-07-02 14:06:01.889587: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-02 14:06:01.892940: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-02 14:06:01.932883: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-02 14:06:01.932923: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-02 14:06:01.932948: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-02 14:06:01.940751: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-02 14:06:02.798331: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, pipeline, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from huggingface_hub import login\n",
    "import evaluate\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# from ray.tune.search.hyperopt import HyperOptSearch\n",
    "# from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b6022c-21c3-4532-aa12-c10c4110e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2438d10-ed93-4005-8110-84f9a48c39fb",
   "metadata": {},
   "source": [
    "# Load and inspect training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4b19a8d-ea5a-4a22-9b16-3f2e1db92d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    309\n",
      "1    309\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('bert_dataset.csv')\n",
    "\n",
    "# Check class distribution\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Balance classes if needed\n",
    "df = df.groupby('label').sample(n=min(df['label'].value_counts()), random_state=42)\n",
    "\n",
    "# Shuffle the dataset\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "df = df[[\"text\", \"label\"]]\n",
    "\n",
    "# Split dataset into test & train\n",
    "df_train = df[128:]\n",
    "df_val = df[:128]\n",
    "\n",
    "tds = Dataset.from_pandas(df_train)\n",
    "vds = Dataset.from_pandas(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb85596-14c7-451a-b4cd-5a24545223b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d6c359-8f59-4d53-b52f-9a8ae1233297",
   "metadata": {},
   "source": [
    "https://medium.com/@fhirfly/fine-tuning-biobert-v1-1-on-a-large-dataset-classifying-medical-queries-c33b4d08ec6a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582603e1-f8ed-431c-b8ec-309a7c5f9697",
   "metadata": {},
   "source": [
    "# Preprocess text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "936dd18c-ad8f-4fa3-8971-cf2ec58a8f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david.yang1/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load BioBERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NeuML/pubmedbert-base-embeddings\")\n",
    "# model = AutoModel.from_pretrained('dmis-lab/biobert-v1.1')\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(df):\n",
    "    return tokenizer(\n",
    "        df['text'],\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        max_length = 512\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a7c2922-c00a-4155-b4f8-2b7b3bb832c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c30933b0b24f4787ad082cb4e751b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94bb53132d774b128687c496011b6eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the tokenizer to the datasets\n",
    "tds = tds.map(tokenize_function, batched=True)\n",
    "vds = vds.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format of the datasets to include only the required columns\n",
    "tds = tds.rename_column('__index_level_0__', 'index').remove_columns(['text', 'index'])\n",
    "vds = vds.rename_column('__index_level_0__', 'index').remove_columns(['text', 'index'])\n",
    "\n",
    "# Define DatasetDict\n",
    "ds = DatasetDict({\n",
    "    \"train\": tds,\n",
    "    \"validation\": vds\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd07a8d-89f1-4c65-96ca-f649ecb71544",
   "metadata": {},
   "source": [
    "# Model fine tuning\n",
    "Parameter tuning: \n",
    "https://kaitchup.substack.com/p/a-guide-on-hyperparameters-and-training\n",
    "https://medium.com/distributed-computing-with-ray/hyperparameter-optimization-for-transformers-a-guide-c4e32c6c989b\n",
    "https://huggingface.co/blog/ray-tune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4991a887-017e-42c9-9348-00fb299443f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "# def model_init():\n",
    "#     return AutoModelForSequenceClassification.from_pretrained(\"NeuML/pubmedbert-base-embeddings\", num_labels=2)\n",
    "\n",
    "# metric = evaluate.load(\"confusion_matrix\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7996c708-67a8-4004-aec1-5201f13dfed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david.yang1/.local/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps=500,\n",
    "    num_train_epochs=3,    # number of training epochs\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_ratio=0.01,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Create the Trainer and start training\n",
    "trainer = Trainer(\n",
    "    args=training_args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"validation\"],\n",
    "    model_init=model_init,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cf95c8-7d23-4e60-ad54-713be0341496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f6180-26a6-4062-89bc-f7459ff4c175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from local folder\n",
    "# train = AutoModelForSequenceClassification.from_pretrained(\"./chunks-pubmed-bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "262235e5-e5d9-4e62-b60f-c251be213d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "eval = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5da6aa0a-2b83-419d-ad02-a73f8d6839b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3069172203540802,\n",
       " 'eval_accuracy': 0.8984375,\n",
       " 'eval_f1': 0.8959999999999999,\n",
       " 'eval_precision': 0.9180327868852459,\n",
       " 'eval_recall': 0.875,\n",
       " 'eval_runtime': 19.0202,\n",
       " 'eval_samples_per_second': 6.73,\n",
       " 'eval_steps_per_second': 0.105}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226fe6c1-2881-49fe-8bfa-1c58c7e5f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./chunks-pubmed-bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a86254-bbae-410c-bdbd-cfe8111cb1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6309a9da-2a63-44e1-a1e6-7f480adc9a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"./chunks-pubmed-bert\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86482e9f-cee7-421f-b90b-e32dc0be10fb",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19e53981-bcdc-4cec-bbf9-90fac21d6aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, tokenizer, max_tokens=512, overlap_sentences=2):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Initialize variables\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_len = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Tokenize the sentence using BERT Tokenizer\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        token_count = len(tokens)\n",
    "\n",
    "        # Finalize the current chunk if adding this sentence exceed token limit\n",
    "        if current_chunk_len + token_count > max_tokens:\n",
    "            text_chunk = \"\".join(current_chunk)\n",
    "            chunks.append(text_chunk)\n",
    "\n",
    "            # Create the next chunk with overlap\n",
    "            overlap_start = max(0, i-overlap_sentences)\n",
    "            current_chunk = []\n",
    "            for j in range(overlap_start, i):\n",
    "                current_chunk.append(sentences[j])\n",
    "            current_chunk_len = len(current_chunk)\n",
    "\n",
    "        # Add the current sentence tokens to the chunk\n",
    "        current_chunk.append(sentence)\n",
    "        current_chunk_len += token_count\n",
    "\n",
    "    # Add the last chunk if it has content\n",
    "    if current_chunk:\n",
    "        text_chunk = \"\".join(current_chunk)\n",
    "        chunks.append(text_chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0566aba-5862-40ef-94f0-b1ec2f0e4d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dataset consisting of chunks of text\n",
    "df_test = df[64:128] \n",
    "# sample = df_test.head(15)\n",
    "sample = df_test.sample(n=15, random_state=42)\n",
    "test_df = df_test.drop(sample.index)\n",
    "\n",
    "# all_chunks = []\n",
    "df_test_chunks = pd.DataFrame()\n",
    "for text in sample[\"text\"]:\n",
    "    chunks = split_text_into_chunks(text, tokenizer)\n",
    "    temp = pd.DataFrame(chunks, columns=[\"text\"])\n",
    "    temp['position'] = temp.index\n",
    "    # all_chunks.extend(chunks)\n",
    "    df_test_chunks = pd.concat([df_test_chunks, temp])\n",
    "\n",
    "df_test_chunks = df_test_chunks.reset_index(drop=True)\n",
    "\n",
    "# df_test_chunks = pd.DataFrame(all_chunks, columns=['text'])\n",
    "\n",
    "df_test_chunks.to_csv('test_chunks_positions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22be07ee-b6e7-4ddf-b3f4-0025c5b3c55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prediction chunks\n",
    "pred_chunks_0 = pd.read_csv(\"0_chunks_labelled.csv\")\n",
    "pred_chunks_1 = pd.read_csv(\"1_chunks_labelled.csv\")\n",
    "pred_chunks_2 = pd.read_csv(\"2_chunks_labelled.csv\")\n",
    "\n",
    "# Concatenate data\n",
    "df_test = pd.concat([pred_chunks_0, pred_chunks_1, pred_chunks_2])\n",
    "\n",
    "# Load dataframe as dataset\n",
    "test = Dataset.from_pandas(df_test)\n",
    "\n",
    "# Tokenize test dataset\n",
    "test = test.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format of the datasets to include only the required columns\n",
    "test = test.rename_column('__index_level_0__', 'index').remove_columns(['text', 'index'])\n",
    "\n",
    "# Define DatasetDict\n",
    "ds_test = DatasetDict({\n",
    "    \"test\": test\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e88cce-fb4f-4e5f-ba23-e1ad97d77f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test performance of the model on labeled chunks\n",
    "pred = trainer.predict(ds_test[\"test\"])\n",
    "pred\n",
    "df_test[\"prediction\"] = pred.predictions.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5929076-3a35-4144-9c8e-d231bdec038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15415a85-050e-4133-8508-0b55564a4962",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\"df_test_chunks.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed2abee-9cea-433a-b50f-fc6c9b437db8",
   "metadata": {},
   "source": [
    "# View model prediction on chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b502af-efe5-4178-8333-e141d5178799",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Given a chunk, provide prediction of the chunk\n",
    "def predict_chunk(chunk, trainer, tokenizer):\n",
    "    tokens = tokenizer.tokenize(chunk)\n",
    "    pred = trainer.predict(tokens)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"------------------------------------------------\")\n",
    "    print(\"GIVEN TEXT: \")\n",
    "    print(chunk)\n",
    "    print(\"================================================\")\n",
    "    print(pred.predictions.argmax(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c38ecea-b0a0-4b6c-b7ee-f6d344eac8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_text_into_chunks(df[\"text\"][0], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd69412b-d46e-41ee-bcd7-ef8875903edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c7c458-e727-4ee2-bb5b-6d06d0f56401",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_df = pd.DataFrame(chunks, columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dc0bed-b71c-4e8d-af6f-b5a8d8fccad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(chunk_df[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a913b82e-a9f4-4294-947c-0dae659529ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_df[\"label\"] = df[\"label\"][0]\n",
    "chunk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee111fd-8d22-459d-895e-e56b7c7415a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Dataset.from_pandas(chunk_df)\n",
    "t = t.map(tokenize_function, batched=True)\n",
    "# t = t.rename_column('__index_level_0__', 'index').remove_columns(['text', 'index'])\n",
    "\n",
    "# Define DatasetDict\n",
    "ds_t = DatasetDict({\n",
    "    \"test\": t\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fdbc2f-4ef0-466a-80bc-e146ac70148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(ds_t[\"test\"])\n",
    "pred.predictions.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc53747-eae5-4c45-8e99-5db27624c55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_chunks(df):\n",
    "    output = pd.DataFrame()\n",
    "    for i, text in enumerate(df[\"text\"]):\n",
    "        if i > 3:\n",
    "            break\n",
    "            \n",
    "        chunks = split_text_into_chunks(text, tokenizer)\n",
    "        \n",
    "        chunks_df = pd.DataFrame(chunks, columns=[\"text\"])\n",
    "        chunks_df[\"label\"] = df[\"label\"][i]\n",
    "        chunks_df[\"position\"] = chunks_df.index\n",
    "\n",
    "        t = Dataset.from_pandas(chunks_df)\n",
    "        t = t.map(tokenize_function, batched=True)\n",
    "        ds_t = DatasetDict({\n",
    "            \"test\": t\n",
    "        })\n",
    "\n",
    "        pred = trainer.predict(ds_t[\"test\"])\n",
    "        chunks_df[\"prediction\"] = pred.predictions.argmax(-1)\n",
    "        output = pd.concat([output, chunks_df], ignore_index=True)\n",
    "        print(chunks_df)\n",
    "        print(\"=========\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43849b29-a61a-41b2-b4a8-7ae4271a12e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prediction_chunks(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62387710-c022-4b50-93e3-9b029f1595cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"chunk_labelled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef2b67-099a-4aa0-8f67-7da5b702d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df_test.sample(n=15, random_state=42)\n",
    "df_test = df_test.drop(sample.index)\n",
    "\n",
    "new_sample = df_test.sample(n=10, random_state=42)\n",
    "\n",
    "# all_chunks = []\n",
    "\n",
    "for text in new_sample[\"text\"]:\n",
    "#     chunks = split_text_into_chunks(text, tokenizer)\n",
    "#     for chunk in chunks:\n",
    "#         predict_chunk(chunk)\n",
    "\n",
    "# df_test_chunks = pd.DataFrame(all_chunks, columns=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876f8168-4232-43d0-97b4-5edc3873e8db",
   "metadata": {},
   "source": [
    "# Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a99f41-086a-4a82-85ca-323077541126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default objective is the sum of all metrics\n",
    "# when metrics are provided, so we have to maximize it.\n",
    "trainer.hyperparameter_search(\n",
    "    direction=\"maximize\", \n",
    "    backend=\"ray\", \n",
    "    n_trials=10 # number of trials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af53c76-d7d1-4141-a712-8de02d3f3d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"ray\",\n",
    "    search_alg=HyperOptSearch(metric=\"objective\", mode=\"max\"),\n",
    "    scheduler=ASHAScheduler(metric=\"objective\", mode=\"max\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf3296-e221-4c71-b017-5fdf02894eee",
   "metadata": {},
   "source": [
    "# ARCHIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac3e18-72b0-44e0-bdf5-59cd7646c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_data(tokenizer, text, max_length):\n",
    "#     encoded = tokenizer.batch_encode_plus(\n",
    "#         text,\n",
    "#         truncation=True,\n",
    "#         padding='longest',\n",
    "#         max_length=max_length,\n",
    "#         return_tensors='pt'  # return PyTorch tensors\n",
    "#     )\n",
    "#     return encoded[\"input_ids\"], encoded[\"attention_mask\"]\n",
    "# # Use an appropriate max_length \n",
    "# input_ids_train, attention_mask_train = encode_data(tokenizer, df_train['text'].tolist(), max_length=512)\n",
    "# input_ids_val, attention_mask_val = encode_data(tokenizer, df_val['text'].tolist(), max_length=512)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
